```{r}
library(dplyr)
moral_machine <- read.csv("../data/moral_machine_small_sample_h2o.csv")


moral_machine <- moral_machine %>% dplyr::select(-X)
```


```{r}
set.seed(2024)
moral_machine$Saved <- as.factor(moral_machine$Saved)

moral_machine_small <- moral_machine[sample(0.25*nrow(moral_machine)),]
head(moral_machine_small)
dropped_DF <- moral_machine_small %>% dplyr::select(-c(AttributeLevel,
                                                ScenarioTypeStrict,
                                                ScenarioOrder,
                                                ScenarioType,
                                                DefaultChoice,
                                                NonDefaultChoice,
                                                DefaultChoiceIsOmission
                                                ))

train_index = sample(0.6 * nrow(moral_machine_small))
train <- dropped_DF[train_index, ]
test <- dropped_DF[-train_index,]

```


```{r}
rm(moral_machine)
```

```{r}
library(randomForest)



tree <- randomForest(Saved ~ ., data=train, importance=TRUE,
                        proximity=TRUE)
```

```{r}
tree


predictions_test <- predict(tree, test)

sum(predictions_test == test$Saved) / nrow(test)

response_matrix <- as.factor(as.numeric(train$Saved))
predcitors_matrix <- train %>% dplyr::select(-Saved) %>% as.matrix()
tune <- tuneRF(x=train %>% dplyr::select(-Saved),
       y=train$Saved,
       ntreeTry = 1000,
       mtryStart=1,
       improve=0.05)

start.time <- Sys.time()

tuned_RF <- randomForest(Saved ~ ., data=train, importance=TRUE,
             proximity=TRUE,
             mtry=4,
             ntree=700,
             replace=T,
             xtest=test %>% dplyr::select(-Saved),
             ytest= test$Saved)

end.time <- Sys.time()

paste((end.time-start.time), "minutes")



plot(tuned_RF, main="Random Forest Classifier Test Set Error Rate vs Num of Trees Grown")
tuned_RF$importance
```

- 73% Test Set Accuracy (non-tuned RF)

```{r}
library(MASS)
library(boot)
start.time2 <- Sys.time()

train_subset <- train %>% dplyr::select(-c(UserCountry3))
logistic_reg <- glm(Saved ~. ,family = binomial, data=train_subset)
end.time2 <- Sys.time()

paste(end.time2-start.time2)

summary(logistic_reg)
# test_subset <- test %>% dplyr::filter(!(UserCountry3 %in% c("GUF", "QAT",
                                                            # "AFG", "DJI", "JEY")))
test_subset <- test

predictions <- predict(logistic_reg, newdata=test_subset, type="response")


sum(as.numeric(predictions > 0.5) == test$Saved) / nrow(test)

AIC(logistic_reg)

logistic_reg$terms
```
```{r}
library(arm)
library(modelsummary)
library(ggplot2)

modelplot(logistic_reg, coef_omit="Intercept", color="darkgreen", size=1) +
  labs(title="Coefficients for logistic regression model")+
  scale_color_brewer(type = 'qual')

modelplot(logistic_reg, exponentiate = TRUE) +
  scale_x_log10() +
  xlab("Odds Ratios and 95% confidence intervals")
```

- 62 Percent test set accuracy

```{r}
tuned_RF$importance[order((tuned_RF$importance[,1] + tuned_RF$importance[,2])/2),1:2]

tuned_RF$importance[order(tuned_RF$importance[,3]),1:3]
tuned_RF$importance[order(tuned_RF$importance[,3]),1:2]


order((tuned_RF$importance[,1] + tuned_RF$importance[,2])/2)

tree
```